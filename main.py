import whisper  
from transformers import MarianMTModel, MarianTokenizer  
import ffmpeg  
import subprocess  
import os  

# --- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ---  
def transcribe_video(video_path):  
    model = whisper.load_model("base")  
    result = model.transcribe(video_path)  
    return result["text"]  

# --- ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ ---  
def translate_text(text, target_lang="ar"):  
    model_name = f"Helsinki-NLP/opus-mt-en-{target_lang}"  
    tokenizer = MarianTokenizer.from_pretrained(model_name)  
    model = MarianMTModel.from_pretrained(model_name)  
    translated = model.generate(**tokenizer(text, return_tensors="pt"))  
    return tokenizer.decode(translated[0], skip_special_tokens=True)  

# --- ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªØ±Ø¬Ù… ---  
def generate_audio(text, output_path="dubbed_audio.wav"):  
    subprocess.run(f"mimic3 --voice ar_JO '{text}' --output-dir {output_path}", shell=True)  

# --- Ø¯Ù…Ø¬ Ø§Ù„ØµÙˆØª Ù…Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ---  
def merge_audio_video(video_path, audio_path, output_path="output.mp4"):  
    (  
        ffmpeg  
        .input(video_path)  
        .output(output_path, vcodec='copy', acodec='aac', strict='experimental')  
        .overwrite_output()  
        .run()  
    )  

# --- Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ---  
if __name__ == "__main__":  
    # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ  
    original_text = transcribe_video("input_video.mp4")  
    # 2. ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ  
    translated_text = translate_text(original_text, target_lang="ar")  
    # 3. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª  
    generate_audio(translated_text, "dubbed_audio.wav")  
    # 4. Ø¯Ù…Ø¬ Ù…Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ  
    merge_audio_video("input_video.mp4", "dubbed_audio.wav", "output_video.mp4")  
    print("ØªÙ…Øª Ø§Ù„Ø¯Ø¨Ù„Ø¬Ø© Ø¨Ù†Ø¬Ø§Ø­! ğŸ‰")  
