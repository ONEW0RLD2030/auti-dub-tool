import whisper
from transformers import MarianMTModel, MarianTokenizer
import subprocess
import ffmpeg

def transcribe_video(video_path):
    model = whisper.load_model("base")
    result = model.transcribe(video_path)
    return result["text"]

def translate_text(text, target_lang="ar"):
    model_name = f"Helsinki-NLP/opus-mt-en-{target_lang}"
    tokenizer = MarianTokenizer.from_pretrained(model_name)
    model = MarianMTModel.from_pretrained(model_name)
    translated = model.generate(**tokenizer(text, return_tensors="pt"))
    return tokenizer.decode(translated[0], skip_special_tokens=True)

def generate_audio(text, output_path="dubbed_audio.wav"):
    subprocess.run(f"mimic3 --voice ar_JO '{text}' --output {output_path}", shell=True)

def merge_audio_video(video_path, audio_path, output_path="output.mp4"):
    (
        ffmpeg
        .input(video_path)
        .output(output_path, vcodec='copy', acodec='aac', strict='experimental')
        .overwrite_output()
        .run()
    )

if __name__ == "__main__":
    # Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ
    original_text = transcribe_video("input.mp4")
    print(f"Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ: {original_text}")

    # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„ØªØ±Ø¬Ù…Ø©
    translated_text = translate_text(original_text, "ar")
    print(f"Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªØ±Ø¬Ù…: {translated_text}")

    # Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª
    generate_audio(translated_text, "dubbed.wav")

    # Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø¯Ù…Ø¬ Ø§Ù„ØµÙˆØª Ù…Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
    merge_audio_video("input.mp4", "dubbed.wav", "output.mp4")
    print("ØªÙ… Ø§Ù„Ø¥Ù†ØªÙ‡Ø§Ø¡! Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø¬Ø¯ÙŠØ¯: output.mp4")import whisper  
from transformers import MarianMTModel, MarianTokenizer  
import ffmpeg  
import subprocess  
import os  

# --- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ---  
def transcribe_video(video_path):  
    model = whisper.load_model("base")  
    result = model.transcribe(video_path)  
    return result["text"]  

# --- ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ ---  
def translate_text(text, target_lang="ar"):  
    model_name = f"Helsinki-NLP/opus-mt-en-{target_lang}"  
    tokenizer = MarianTokenizer.from_pretrained(model_name)  
    model = MarianMTModel.from_pretrained(model_name)  
    translated = model.generate(**tokenizer(text, return_tensors="pt"))  
    return tokenizer.decode(translated[0], skip_special_tokens=True)  

# --- ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªØ±Ø¬Ù… ---  
def generate_audio(text, output_path="dubbed_audio.wav"):  
    subprocess.run(f"mimic3 --voice ar_JO '{text}' --output-dir {output_path}", shell=True)  

# --- Ø¯Ù…Ø¬ Ø§Ù„ØµÙˆØª Ù…Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ---  
def merge_audio_video(video_path, audio_path, output_path="output.mp4"):  
    (  
        ffmpeg  
        .input(video_path)  
        .output(output_path, vcodec='copy', acodec='aac', strict='experimental')  
        .overwrite_output()  
        .run()  
    )  

# --- Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ---  
if __name__ == "__main__":  
    # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ  
    original_text = transcribe_video("input_video.mp4")  
    # 2. ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ  
    translated_text = translate_text(original_text, target_lang="ar")  
    # 3. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª  
    generate_audio(translated_text, "dubbed_audio.wav")  
    # 4. Ø¯Ù…Ø¬ Ù…Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ  
    merge_audio_video("input_video.mp4", "dubbed_audio.wav", "output_video.mp4")  
    print("ØªÙ…Øª Ø§Ù„Ø¯Ø¨Ù„Ø¬Ø© Ø¨Ù†Ø¬Ø§Ø­! ğŸ‰")  
